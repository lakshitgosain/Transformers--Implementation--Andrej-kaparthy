{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNCoiYvw5P9OI0IXoCKPOb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakshitgosain/Transformers--Implementation--Andrej-kaparthy/blob/main/Transformers_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT - Generative Pretrained Transformers\n",
        "- It is a base from Attention is all you need\n",
        "- In this tutorial, we are not going to recreate what's there for ChatGPT as it's a very high level, production grade system that involves training on whole of the internet's data(most of not whole) and involves rigorous pretraining methods"
      ],
      "metadata": {
        "id": "K1XCYqzfwkMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We will use tiny shakespere"
      ],
      "metadata": {
        "id": "x9rH-gWbxHKS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to create a character level transformer here and it's going to predict the next character.\n",
        "- In chatgpt, the output is based of token level prediction. It predicts next token(word) and not the next character"
      ],
      "metadata": {
        "id": "3lY2N6Qz_3dk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/karpathy/nanoGPT - Repo for training transformers on any given text .\n",
        "We are defining/creating the nonogpt from scratch"
      ],
      "metadata": {
        "id": "kbahj9wkAFJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSzmKxCKCAmN",
        "outputId": "1af5d80c-8916-41a7-8ae4-3d6a87abe819"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-02 08:20:50--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-01-02 08:20:50 (20.8 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"input.txt\",'r') as f:\n",
        "  text=f.read()"
      ],
      "metadata": {
        "id": "whD7kbRPQtai"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of the text is :\", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXnuLiUsXKyy",
        "outputId": "776f90c9-db7b-4e99-b0db-6fe1dd665214"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of the text is : 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's look at the first 1000 characters\n",
        "print(\"First 1000 characters\", text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RU9QonTKXQZo",
        "outputId": "3acdea57-2e88-4715-8993-22d421799fed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 1000 characters First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars=(sorted(list(set(text))))\n",
        "vocab_size=len(chars)\n",
        "print(\"\".join(chars)) #All the characters that have been used in the dataset\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6dseDbTXfAp",
        "outputId": "71b7e1cb-3211-464c-dd42-619ad74e6dad"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Strategy ome strategy to tokenize the text. We will be translating characters to integers.\n",
        "Google uses SentencePiece.\n",
        "OpenAI uses Tiktoken. THis has 50000 tokens. We in our use case have 65 tokens as we saw above.\n"
      ],
      "metadata": {
        "id": "Zp5SP7eYYNri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stoi={ch:i for i,ch in enumerate(chars)}\n",
        "itos={i:ch for i,ch in enumerate(chars)}\n",
        "encode= lambda s: [stoi[c] for c in s] # Encoder: takes a stering and outputs a list of integers\n",
        "decode= lambda l: \"\".join([itos[i] for i in l])"
      ],
      "metadata": {
        "id": "UIXX-F2_ZQ-p"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode(\"Hello There\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fyil0Mn8ZuRc",
        "outputId": "d6586999-90d3-4c34-b677-68074e3650f0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[20, 43, 50, 50, 53, 1, 32, 46, 43, 56, 43]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "ZgPvMd-CZ2Jg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=torch.tensor(encode(text),dtype=torch.long)\n",
        "print(data.shape,data.dtype)\n",
        "print(data[:1000]) #The 1000 characters we looked at earlier will be the GPT looks like this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-k8n6c2aHjD",
        "outputId": "1a324eaf-bd15-4994-bee0-e11d1d707af7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The whole dataset is now represented as a very large sequence of integers above"
      ],
      "metadata": {
        "id": "ADFVEFKQad6A"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n=int(0.9*len(data))\n",
        "train_data=data[:n]\n",
        "val_data=data[n:]"
      ],
      "metadata": {
        "id": "BUXsKjfbaxHQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We will now start plugging the text into the transformers model for it to learn. We will do it in chunks and train chunks at a time."
      ],
      "metadata": {
        "id": "rvvxPkNmb0Af"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pKQ6jH1b70L",
        "outputId": "61f016a0-3a82-4223-f473-d6b391cc3f7d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=train_data[:block_size]\n",
        "y=train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context=x[:t+1]\n",
        "  target=y[t]\n",
        "  print(f\"when the input is {context} the target is {target}\")\n",
        "  #Look at the output of the for loop. this is the reason why we offset to +1 as we need the output of the tensor also present\n",
        "  #We train on the block size nopt just for efficiency but also to make the transformer to see the context from all the way to 1 to block size and see everythin in between.\n",
        "  #Transformer knows how to predict everything upto block size and know how to predict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dm2hdZUcDaK",
        "outputId": "7dbedf80-70f9-4d95-82ef-77b725a7adba"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when the input is tensor([18]) the target is 47\n",
            "when the input is tensor([18, 47]) the target is 56\n",
            "when the input is tensor([18, 47, 56]) the target is 57\n",
            "when the input is tensor([18, 47, 56, 57]) the target is 58\n",
            "when the input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
            "when the input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
            "when the input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
            "when the input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We will have batches of the blocks/chunks of characters for efficiency so that we keep the CPU/GPU busy and train parallely on those batches\n"
      ],
      "metadata": {
        "id": "0saVzyPRcoI6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size=4 #How many independent sequences will be processed in parallel\n",
        "block_size=8 # What is the maximum context length of predictions\n",
        "\n",
        "def get_batch(split):\n",
        "  data= train_data if split==\"train\" else val_data\n",
        "  ix=torch.randint(len(data) - block_size, (batch_size,)) #offsets into the trainiing set\n",
        "  print(\"ix\",ix)\n",
        "  x=torch.stack([data[i:i+block_size] for i in ix]) #Concatenates a sequence of tensors along a new dimension (dim), all tensors must be same size.\n",
        "  y=torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x,y\n",
        "\n",
        "xb,yb= get_batch(\"train\")\n",
        "print(\"inputs:\")\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print(\"Targets: \")\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print(\"------\")\n",
        "\n",
        "for b in range(batch_size): #Batch Dimension\n",
        "  for t in range(block_size): #Time dimension\n",
        "    context=xb[b, :t+1]\n",
        "    target=yb[b,t]\n",
        "    print(f\"when input is {context.tolist()} the target is {target}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM0Pla4edYKz",
        "outputId": "05573994-b83f-4992-bd43-2f0f55710fbe"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ix tensor([ 76049, 234249, 934904, 560986])\n",
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "Targets: \n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "------\n",
            "when input is [24] the target is 43\n",
            "when input is [24, 43] the target is 58\n",
            "when input is [24, 43, 58] the target is 5\n",
            "when input is [24, 43, 58, 5] the target is 57\n",
            "when input is [24, 43, 58, 5, 57] the target is 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target is 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target is 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target is 39\n",
            "when input is [44] the target is 53\n",
            "when input is [44, 53] the target is 56\n",
            "when input is [44, 53, 56] the target is 1\n",
            "when input is [44, 53, 56, 1] the target is 58\n",
            "when input is [44, 53, 56, 1, 58] the target is 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target is 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target is 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target is 1\n",
            "when input is [52] the target is 58\n",
            "when input is [52, 58] the target is 1\n",
            "when input is [52, 58, 1] the target is 58\n",
            "when input is [52, 58, 1, 58] the target is 46\n",
            "when input is [52, 58, 1, 58, 46] the target is 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target is 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target is 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target is 46\n",
            "when input is [25] the target is 17\n",
            "when input is [25, 17] the target is 27\n",
            "when input is [25, 17, 27] the target is 10\n",
            "when input is [25, 17, 27, 10] the target is 0\n",
            "when input is [25, 17, 27, 10, 0] the target is 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target is 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target is 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target is 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The above contains 32 examples. When the input 1 token which is being predicted.\n"
      ],
      "metadata": {
        "id": "IuqYhE5ndlGT"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We now have a batch of input we need to input to the transformer model.\n"
      ],
      "metadata": {
        "id": "XvIhZ-SApDat"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We will use the bigram language model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "id": "IrQTadpEpO93"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}